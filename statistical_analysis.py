"""
çµ±è¨ˆæ¤œå®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ­£è¦æ€§ã®æ¤œå®šã€Wilcoxonæ¤œå®šã€å¤šé‡æ¯”è¼ƒã€Cohen's dã®è¨ˆç®—ã‚’è¡Œã„ã¾ã™
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import shapiro, wilcoxon, mannwhitneyu, friedmanchisquare
from statsmodels.stats.multitest import multipletests
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import itertools


def test_normality(data: np.ndarray, alpha: float = 0.05) -> Tuple[bool, float, float]:
    """
    æ­£è¦æ€§ã®æ¤œå®šï¼ˆShapiro-Wilk testï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™
    
    Parameters:
    -----------
    data : np.ndarray
        æ¤œå®šã™ã‚‹ãƒ‡ãƒ¼ã‚¿
    alpha : float
        æœ‰æ„æ°´æº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
    
    Returns:
    --------
    Tuple[bool, float, float]
        (æ­£è¦æ€§ã‚ã‚Šã‹ã©ã†ã‹, çµ±è¨ˆé‡, på€¤)
    """
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒ3æœªæº€ã®å ´åˆã¯æ¤œå®šä¸å¯
    if len(data) < 3:
        return False, np.nan, np.nan
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒ5000ã‚’è¶…ãˆã‚‹å ´åˆã¯æœ€åˆã®5000å€‹ã®ã¿ã‚’ä½¿ç”¨
    if len(data) > 5000:
        data = data[:5000]
    
    statistic, p_value = shapiro(data)
    is_normal = p_value > alpha
    
    return is_normal, statistic, p_value


def create_qq_plot(data: np.ndarray, ax: Optional[plt.Axes] = None, 
                   title: str = 'Q-Q Plot') -> plt.Axes:
    """
    Q-Q plotã‚’ä½œæˆã—ã¾ã™
    
    Parameters:
    -----------
    data : np.ndarray
        ãƒ‡ãƒ¼ã‚¿
    ax : Optional[plt.Axes]
        ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹è»¸ï¼ˆNoneã®å ´åˆã¯æ–°è¦ä½œæˆï¼‰
    title : str
        ãƒ—ãƒ­ãƒƒãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«
    
    Returns:
    --------
    plt.Axes
        ãƒ—ãƒ­ãƒƒãƒˆã•ã‚ŒãŸè»¸
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(6, 6))
    
    stats.probplot(data, dist="norm", plot=ax)
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    
    return ax


def calculate_cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:
    """
    Cohen's dã‚’è¨ˆç®—ã—ã¾ã™
    
    Parameters:
    -----------
    group1 : np.ndarray
        ã‚°ãƒ«ãƒ¼ãƒ—1ã®ãƒ‡ãƒ¼ã‚¿
    group2 : np.ndarray
        ã‚°ãƒ«ãƒ¼ãƒ—2ã®ãƒ‡ãƒ¼ã‚¿
    
    Returns:
    --------
    float
        Cohen's d
    """
    n1, n2 = len(group1), len(group2)
    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
    
    # ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸæ¨™æº–åå·®
    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
    
    if pooled_std == 0:
        return 0.0
    
    # Cohen's d
    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std
    
    return cohens_d


def perform_wilcoxon_test(group1: np.ndarray, group2: np.ndarray, 
                         alternative: str = 'two-sided') -> Tuple[float, float]:
    """
    Wilcoxonæ¤œå®šã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆå¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼‰
    
    Parameters:
    -----------
    group1 : np.ndarray
        ã‚°ãƒ«ãƒ¼ãƒ—1ã®ãƒ‡ãƒ¼ã‚¿
    group2 : np.ndarray
        ã‚°ãƒ«ãƒ¼ãƒ—2ã®ãƒ‡ãƒ¼ã‚¿
    alternative : str
        æ¤œå®šã®ç¨®é¡ï¼ˆ'two-sided', 'less', 'greater'ï¼‰
    
    Returns:
    --------
    Tuple[float, float]
        (çµ±è¨ˆé‡, på€¤)
    """
    if len(group1) != len(group2):
        # å¯¾å¿œã®ãªã„ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Mann-Whitney Uæ¤œå®šã‚’ä½¿ç”¨
        statistic, p_value = mannwhitneyu(group1, group2, alternative=alternative)
    else:
        # å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Wilcoxonæ¤œå®šã‚’ä½¿ç”¨
        statistic, p_value = wilcoxon(group1, group2, alternative=alternative)
    
    return statistic, p_value


def multiple_comparison_holm(p_values: List[float], alpha: float = 0.05) -> List[bool]:
    """
    Holmè£œæ­£ã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒã‚’è¡Œã„ã¾ã™
    
    Parameters:
    -----------
    p_values : List[float]
        på€¤ã®ãƒªã‚¹ãƒˆ
    alpha : float
        æœ‰æ„æ°´æº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
    
    Returns:
    --------
    List[bool]
        å„æ¯”è¼ƒãŒæœ‰æ„ã‹ã©ã†ã‹ã®ãƒªã‚¹ãƒˆ
    """
    rejected, p_adjusted, _, _ = multipletests(p_values, alpha=alpha, method='holm')
    return rejected.tolist()


def format_p_value(p_value: float) -> str:
    """
    på€¤ã‚’æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™
    
    Parameters:
    -----------
    p_value : float
        på€¤
    
    Returns:
    --------
    str
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸpå€¤
    """
    if p_value < 0.001:
        return "<0.001"
    elif p_value < 0.01:
        return f"{p_value:.4f}"
    else:
        return f"{p_value:.4f}"


def analyze_group_comparisons(data: pd.DataFrame, value_col: str, 
                              method_col: str = 'Method',
                              methods: Optional[List[str]] = None) -> pd.DataFrame:
    """
    ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®æ¯”è¼ƒã‚’è¡Œã„ã€çµ±è¨ˆæ¤œå®šã®çµæœã‚’è¿”ã—ã¾ã™
    
    Parameters:
    -----------
    data : pd.DataFrame
        ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    value_col : str
        å€¤ã®åˆ—å
    method_col : str
        ãƒ¡ã‚½ãƒƒãƒ‰åˆ—åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'Method'ï¼‰
    methods : Optional[List[str]]
        æ¯”è¼ƒã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
    
    Returns:
    --------
    pd.DataFrame
        æ¤œå®šçµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if methods is None:
        methods = data[method_col].unique().tolist()
    
    results = []
    
    # å…¨ã¦ã®ãƒšã‚¢ã®çµ„ã¿åˆã‚ã›
    for method1, method2 in itertools.combinations(methods, 2):
        group1 = data[data[method_col] == method1][value_col].dropna().values
        group2 = data[data[method_col] == method2][value_col].dropna().values
        
        if len(group1) == 0 or len(group2) == 0:
            continue
        
        # çµ±è¨ˆé‡ã‚’è¨ˆç®—
        mean1, std1 = np.mean(group1), np.std(group1, ddof=1)
        mean2, std2 = np.mean(group2), np.std(group2, ddof=1)
        
        # Wilcoxonæ¤œå®š
        statistic, p_value = perform_wilcoxon_test(group1, group2)
        
        # Cohen's d
        cohens_d = calculate_cohens_d(group1, group2)
        
        # åŠ¹æœé‡ã®è§£é‡ˆ
        if abs(cohens_d) < 0.2:
            effect_size = "negligible"
        elif abs(cohens_d) < 0.5:
            effect_size = "small"
        elif abs(cohens_d) < 0.8:
            effect_size = "medium"
        else:
            effect_size = "large"
        
        # æ–¹å‘æ€§ã®åˆ¤å®š
        if mean1 > mean2:
            direction = f"{method1} > {method2}"
        else:
            direction = f"{method2} > {method1}"
        
        results.append({
            'method1': method1,
            'method2': method2,
            'direction': direction,
            'mean1': mean1,
            'std1': std1,
            'mean2': mean2,
            'std2': std2,
            'statistic': statistic,
            'p_value': p_value,
            'cohens_d': cohens_d,
            'effect_size': effect_size
        })
    
    return pd.DataFrame(results)


def perform_friedman_test(data: pd.DataFrame, value_col: str, 
                          method_col: str = 'Method',
                          methods: Optional[List[str]] = None) -> Tuple[float, float]:
    """
    Friedmanæ¤œå®šã‚’å®Ÿè¡Œã—ã¾ã™
    
    Parameters:
    -----------
    data : pd.DataFrame
        ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆå¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’æƒ³å®šï¼‰
    value_col : str
        å€¤ã®åˆ—å
    method_col : str
        ãƒ¡ã‚½ãƒƒãƒ‰åˆ—å
    methods : Optional[List[str]]
        æ¯”è¼ƒã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰
    
    Returns:
    --------
    Tuple[float, float]
        (çµ±è¨ˆé‡, på€¤)
    """
    if methods is None:
        methods = data[method_col].unique().tolist()
    
    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    method_data_list = []
    for method in methods:
        method_data = data[data[method_col] == method][value_col].dropna().values
        if len(method_data) > 0:
            method_data_list.append(method_data)
    
    if len(method_data_list) < 3:
        return np.nan, 1.0
    
    # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’æƒãˆã‚‹ï¼ˆæœ€çŸ­ã®é•·ã•ã«åˆã‚ã›ã‚‹ï¼‰
    min_len = min(len(d) for d in method_data_list)
    method_data_list = [d[:min_len] for d in method_data_list]
    
    # Friedmanæ¤œå®šã‚’å®Ÿè¡Œ
    try:
        statistic, p_value = friedmanchisquare(*method_data_list)
    except ValueError:
        # ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆ
        return np.nan, 1.0
    
    return statistic, p_value


def apply_holm_correction(results_df: pd.DataFrame, alpha: float = 0.05) -> pd.DataFrame:
    """
    Holmè£œæ­£ã‚’é©ç”¨ã—ã¾ã™
    
    Parameters:
    -----------
    results_df : pd.DataFrame
        æ¤œå®šçµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    alpha : float
        æœ‰æ„æ°´æº–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05ï¼‰
    
    Returns:
    --------
    pd.DataFrame
        Holmè£œæ­£å¾Œã®çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    p_values = results_df['p_value'].values
    rejected = multiple_comparison_holm(p_values, alpha=alpha)
    
    results_df = results_df.copy()
    results_df['significant'] = rejected
    results_df['p_adjusted'] = multipletests(p_values, alpha=alpha, method='holm')[1]
    
    return results_df


def format_result_string(row: pd.Series, data_name: str = '') -> str:
    """
    æ¤œå®šçµæœã‚’æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™
    
    Parameters:
    -----------
    row : pd.Series
        æ¤œå®šçµæœã®è¡Œ
    data_name : str
        ãƒ‡ãƒ¼ã‚¿åï¼ˆä¾‹: 'NASA-TLX total'ï¼‰
    
    Returns:
    --------
    str
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸçµæœæ–‡å­—åˆ—
    """
    method1 = row['method1']
    method2 = row['method2']
    mean1 = row['mean1']
    std1 = row['std1']
    mean2 = row['mean2']
    std2 = row['std2']
    p_value = row['p_value']
    cohens_d = row['cohens_d']
    
    # æ–¹å‘æ€§ã«å¿œã˜ã¦é †åºã‚’æ±ºå®š
    if mean1 > mean2:
        direction = f"{method1} > {method2}"
        mean_high = mean1
        std_high = std1
        mean_low = mean2
        std_low = std2
    else:
        direction = f"{method2} > {method1}"
        mean_high = mean2
        std_high = std2
        mean_low = mean1
        std_low = std1
    
    p_str = format_p_value(p_value)
    
    result = f"{data_name} {direction} {mean_high:.2f} Â± {std_high:.2f} vs {mean_low:.2f} Â± {std_low:.2f} {p_str} ğ‘‘ğ‘§ = {cohens_d:.2f}"
    
    return result


def save_statistical_results(results_df: pd.DataFrame, output_path: Path, 
                            data_name: str = ''):
    """
    çµ±è¨ˆæ¤œå®šã®çµæœã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™
    
    Parameters:
    -----------
    results_df : pd.DataFrame
        æ¤œå®šçµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    output_path : Path
        å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    data_name : str
        ãƒ‡ãƒ¼ã‚¿å
    """
    with open(output_path, 'a', encoding='utf-8') as f:
        f.write(f"\n{'='*80}\n")
        f.write(f"{data_name}\n")
        f.write(f"{'='*80}\n\n")
        
        # å„æ¯”è¼ƒã®æœ‰æ„æ€§ãƒ¬ãƒ™ãƒ«ã‚’å–å¾—ã—ã€æœ€å¤§ã®*æ•°ã‚’æ±ºå®š
        max_level = 0
        for _, row in results_df.iterrows():
            p_value = row.get('p_adjusted', row.get('p_value', 1.0))
            level, _ = get_significance_level(p_value)
            max_level = max(max_level, level)
        
        # æœ€å¤§ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸ*è¨˜å·ã‚’æ±ºå®š
        if max_level >= 99:
            max_symbol = '***'
        elif max_level >= 95:
            max_symbol = '**'
        elif max_level >= 90:
            max_symbol = '*'
        else:
            max_symbol = ''
        
        for _, row in results_df.iterrows():
            result_str = format_result_string(row, data_name)
            # æœ‰æ„æ€§ãŒã‚ã‚‹å ´åˆã®ã¿*ã‚’è¿½åŠ 
            p_value = row.get('p_adjusted', row.get('p_value', 1.0))
            level, symbol = get_significance_level(p_value)
            if level > 0 and symbol == max_symbol:
                result_str = f"{max_symbol} {result_str}"
            f.write(f"{result_str}\n")
        
        f.write("\n")


def save_normality_test_result(data_name: str, is_normal: bool, 
                               statistic: float, p_value: float, 
                               output_path: Path):
    """
    æ­£è¦æ€§æ¤œå®šã®çµæœã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™
    
    Parameters:
    -----------
    data_name : str
        ãƒ‡ãƒ¼ã‚¿å
    is_normal : bool
        æ­£è¦æ€§ã‚ã‚Šã‹ã©ã†ã‹
    statistic : float
        çµ±è¨ˆé‡
    p_value : float
        på€¤
    output_path : Path
        å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    with open(output_path, 'a', encoding='utf-8') as f:
        # ãƒ‡ãƒ¼ã‚¿åã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦å‡ºåŠ›
        f.write(f"\n{'='*80}\n")
        f.write(f"{data_name}\n")
        f.write(f"{'='*80}\n\n")
        
        # æ­£è¦æ€§æ¤œå®šã®çµæœã‚’å‡ºåŠ›
        if np.isnan(statistic) or np.isnan(p_value):
            f.write(f"{data_name} ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚æ¤œå®šä¸å¯\n")
        else:
            normal_str = "æ­£è¦åˆ†å¸ƒã«å¾“ã†" if is_normal else "æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„"
            p_str = format_p_value(p_value)
            f.write(f"{data_name} Shapiro-Wilk statistic={statistic:.4f}, p-value={p_str}, {normal_str}\n")
        
        f.write("\n")


def get_significance_level(p_value: float) -> Tuple[int, str]:
    """
    på€¤ã‹ã‚‰æœ‰æ„æ°´æº–ã¨è¨˜å·ã‚’è¿”ã—ã¾ã™
    
    Parameters:
    -----------
    p_value : float
        på€¤
    
    Returns:
    --------
    Tuple[int, str]
        (æœ‰æ„æ°´æº–ã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ, è¨˜å·)
    """
    if p_value < 0.01:  # 99%
        return 99, '***'
    elif p_value < 0.05:  # 95%
        return 95, '**'
    elif p_value < 0.10:  # 90%
        return 90, '*'
    else:
        return 0, 'ns'

